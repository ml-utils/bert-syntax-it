# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PKYTH2q2bncPfBxLnZg5LQ7fCbklBkCc
"""

#!git clone https://github.com/ml-utils/bert-syntax-it.git

#!pip install folium==0.2.1
#!pip install pytorch-pretrained-bert

import os.path
from collections import Counter
import json
import argparse, sys
import csv

import torch
from pytorch_pretrained_bert import BertForMaskedLM,tokenization
from torch.nn.functional import softmax
from pytorch_pretrained_bert.tokenization import BertTokenizer
from pytorch_pretrained_bert.modeling import BertPreTrainedModel

import bert_utils
from bert_utils import load_testset_data, analize_sentence, get_probs_for_words, tokenize_sentence, \
    estimate_sentence_probability_from_text


def load_it():
    cc = Counter()
    # note: I edited the LM_Syneval/src/make_templates.py script, and run "python LM_Syneval/src/make_templates.py LM_Syneval/data/templates/ > marvin_linzen_dataset.tsv"
    out = []
    for line in open("it_dataset.tsv"):
        case = line.strip().split("\t")
        cc[case[1]]+=1
        g,ug = case[-2],case[-1]
        g = g.split()
        ug = ug.split()
        assert(len(g)==len(ug)),(g,ug)
        diffs = [i for i,pair in enumerate(zip(g,ug)) if pair[0]!=pair[1]]
        if (len(diffs)!=1):
            #print(diffs)
            #print(g,ug)
            continue
        assert(len(diffs)==1),diffs
        gv=g[diffs[0]]   # good
        ugv=ug[diffs[0]] # bad
        g[diffs[0]]="***mask***"
        g.append(".")
        out.append((case[0],case[1]," ".join(g),gv,ugv))
    return out

def load_marvin():
    cc = Counter()
    # note: I edited the LM_Syneval/src/make_templates.py script, and run "python LM_Syneval/src/make_templates.py LM_Syneval/data/templates/ > marvin_linzen_dataset.tsv"
    out = []
    for line in open("marvin_linzen_dataset.tsv"):
        case = line.strip().split("\t")
        cc[case[1]]+=1
        g,ug = case[-2],case[-1]
        g = g.split()
        ug = ug.split()
        assert(len(g)==len(ug)),(g,ug)
        diffs = [i for i,pair in enumerate(zip(g,ug)) if pair[0]!=pair[1]]
        if (len(diffs)!=1):
            #print(diffs)
            #print(g,ug)
            continue    
        assert(len(diffs)==1),diffs
        gv=g[diffs[0]]   # good
        ugv=ug[diffs[0]] # bad
        g[diffs[0]]="***mask***"
        g.append(".")
        out.append((case[0],case[1]," ".join(g),gv,ugv))
    return out

class bcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'

def eval_it(bert,tokenizer):
    o = load_it()
    print(len(o), file=sys.stderr)
    from collections import defaultdict
    import time
    rc = defaultdict(Counter)
    tc = Counter()
    start = time.time()
    print(f'{bcolors.WARNING}{len(o)} sentences to process..{bcolors.ENDC}')
    for i, (case, tp, s, good_word, bad_word) in enumerate(o):
        ps = get_probs_for_words(bert, tokenizer, s, good_word, bad_word)
        if ps is None: ps = [0, 1]
        gp = ps[0]
        bp = ps[1]
        print(gp > bp, case, tp, good_word, bad_word, s)
        if i % 100 == 0:
            print(f'{bcolors.WARNING}{i}{bcolors.ENDC}')
            print(i, time.time() - start, file=sys.stderr)
            start = time.time()
            sys.stdout.flush()

def eval_marvin(bert,tokenizer):
    o = load_marvin()
    print(len(o),file=sys.stderr)
    from collections import defaultdict
    import time
    rc = defaultdict(Counter)
    tc = Counter()
    start = time.time()
    for i,(case,tp,s,g,b) in enumerate(o):
        ps = get_probs_for_words(bert,tokenizer,s,g,b)
        if ps is None: ps = [0,1]
        gp = ps[0]
        bp = ps[1]
        print(gp>bp,case,tp,g,b,s)
        if i % 100==0:
            print(i,time.time()-start,file=sys.stderr)
            start=time.time()
            sys.stdout.flush()

def eval_lgd(bert,tokenizer):
    for i,line in enumerate(open("lgd_dataset_with_is_are.tsv",encoding="utf8")):
        na,_,masked,good,bad = line.strip().split("\t")
        ps = get_probs_for_words(bert,tokenizer,masked,good,bad)
        if ps is None: continue
        gp = ps[0]
        bp = ps[1]
        print(str(gp>bp),na,good,gp,bad,bp,masked.encode("utf8"),sep=u"\t")
        if i%100 == 0:
            print(i,file=sys.stderr)
            sys.stdout.flush()


def read_gulordava():
    rows = csv.DictReader(open("generated.tab",encoding="utf8"),delimiter="\t")
    data=[]
    for row in rows:
        row2=next(rows)
        assert(row['sent']==row2['sent'])
        assert(row['class']=='correct')
        assert(row2['class']=='wrong')
        sent = row['sent'].lower().split()[:-1] # dump the <eos> token.
        good_form = row['form']
        bad_form  = row2['form']
        sent[int(row['len_prefix'])]="***mask***"
        sent = " ".join(sent)
        data.append((sent,row['n_attr'],good_form,bad_form))
    return data


def eval_gulordava(bert,tokenizer):
    for i,(masked,natt,good,bad) in enumerate(read_gulordava()):
        if good in ["is","are"]:
            print("skipping is/are")
            continue
        ps = get_probs_for_words(bert,tokenizer,masked,good,bad)
        if ps is None: continue
        gp = ps[0]
        bp = ps[1]
        print(str(gp>bp),natt,good,gp,bad,bp,masked.encode("utf8"),sep=u"\t")
        if i%100 == 0:
            print(i,file=sys.stderr)
            sys.stdout.flush()

# choose_eval()


def init_bert_model(model_name, dict_name=None, do_lower_case=False) -> (BertPreTrainedModel, BertTokenizer):
    # model_name = 'bert-large-uncased'
    #if 'base' in sys.argv: model_name = 'bert-base-uncased'
    print(f'model_name: {model_name}')
    print("loading model:", model_name, file=sys.stderr)
    bert = BertForMaskedLM.from_pretrained(model_name)
    print("bert model loaded, getting the tokenizer..")

    if dict_name is None:
        vocab_filepath = model_name
    else:
        vocab_filepath = os.path.join(model_name, 'dict.txt')
    tokenizer = tokenization.BertTokenizer.from_pretrained(vocab_filepath, do_lower_case=do_lower_case)

    print("tokenizer ready.")

    bert.eval()
    return bert, tokenizer


def run_eval(eval_suite, bert: BertPreTrainedModel, tokenizer: BertTokenizer):
    print(f'running eval, eval_suite: {eval_suite}')
    if 'it' == eval_suite:
        eval_it(bert, tokenizer)
    elif 'marvin' == eval_suite:
        eval_marvin(bert,tokenizer)
    elif 'gul' == eval_suite:
        eval_gulordava(bert,tokenizer)
    else:
        eval_lgd(bert,tokenizer)


def arg_parse():
    print('parsing args..')
    # Python program to demonstrate
    # command line arguments

    import getopt, sys

    # Remove 1st argument from the
    # list of command line arguments
    argumentList = sys.argv[1:]

    options = "be:"

    # Long options
    long_options = ["bert_model", "eval_suite"]

    DEFAULT_MODEL = 'bert-large-uncased'
    DEFAULT_EVAL_SUITE = 'lgd'
    model_name = DEFAULT_MODEL
    eval_suite = DEFAULT_EVAL_SUITE

    try:
        # Parsing argument
        print(f'argumentList: {argumentList}')

        # checking each argument
        for arg_idx, currentArgument  in enumerate(argumentList):
            print(f'persing currentArgument {currentArgument}')
            if currentArgument in ("-h", "--Help"):
                print("Displaying Help")

            elif currentArgument in ("-b", "--bert_model"):

                argValue = argumentList[arg_idx+1]
                print(f'currentArgument: {currentArgument}, argValue: {argValue}')
                if argValue == 'base':
                    model_name = 'bert-base-uncased'
                else:
                    model_name = argValue
                    print(f'set model_name: {model_name}')

            elif currentArgument in ("-e", "--eval_suite"):
                argValue = argumentList[arg_idx + 1]
                print(f'currentArgument: {currentArgument}, argValue: {argValue}')
                eval_suite = argValue

    except getopt.error as err:
        # output error, and return with an error code
        print(str(err))

    print(f'model_name {model_name}, eval_suite {eval_suite}')
    return model_name, eval_suite


import numpy as np
from scipy.special import softmax



def get_masked_word_probability(bert, tokenizer):
    return 0


def custom_eval(sentence, bert, tokenizer):
    bert, tokenizer = init_bert_model('bert-base-uncased')

    compare_tokens, compare_target_idx = tokenize_sentence(tokenizer, "What is ***your*** name?")

    bare_sentence_tokens = tokenizer.tokenize(sentence)

    paper_logprobs = bert_get_logprobs(bare_sentence_tokens, None, bert, tokenizer, device=None)

    tokens_list = ['[CLS]'] + bare_sentence_tokens + ['[SEP]']
    target_idx = 3
    masked_word = tokens_list[target_idx]
    tokens_list[target_idx] = '[MASK]'

    print(f'tokens: {tokens_list}, masked_word: {masked_word}')
    print(f'compare_tokens: {compare_tokens}')

    input_ids = tokenizer.convert_tokens_to_ids(tokens_list)

    try:
        masked_word_id = tokenizer.convert_tokens_to_ids([masked_word])
    except KeyError:
        print(f"unable to convert {masked_word} to id")
        return None
    tens = torch.LongTensor(input_ids).unsqueeze(0)

    res_unsliced = bert(tens)
    res=res_unsliced[0, target_idx]

    # res=torch.nn.functional.softmax(res,-1)

    pred = bert("What is [MASK] name?")

    # Set the maximum sequence length.
    MAX_LEN = 128
    # Pad our input tokens
    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],
                              maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")
    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary
    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]
    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype="long", truncating="post", padding="post")


def print_sentence_pairs_probabilities(bert: BertPreTrainedModel, tokenizer: BertTokenizer, sentence_data):
    sentence_good_no_extraction = sentence_data['sentence_good_no_extraction']
    sentence_bad_extraction = sentence_data['sentence_bad_extraction']
    sentence_good_extraction_resumption = sentence_data['sentence_good_extraction_resumption']
    sentence_good_extraction_as_subject = sentence_data['sentence_good_extraction_as_subject']
    print(f'sentence_good_no_extraction: {sentence_good_no_extraction}')
    print(f'sentence_bad_extraction: {sentence_bad_extraction}')
    print(f'sentence_good_extraction_resumption: {sentence_good_extraction_resumption}')
    print(f'sentence_good_extraction_as_subject: {sentence_good_extraction_as_subject}')

    prob_sentence_good_no_extraction = estimate_sentence_probability_from_text(bert, tokenizer, sentence_good_no_extraction)
    prob_sentence_bad_extraction = estimate_sentence_probability_from_text(bert, tokenizer, sentence_bad_extraction)
    prob_sentence_good_extraction_resumption = estimate_sentence_probability_from_text(bert, tokenizer, sentence_good_extraction_resumption)
    prob_sentence_good_extraction_as_subject = estimate_sentence_probability_from_text(bert, tokenizer, sentence_good_extraction_as_subject)

    print(f'prob_sentence_good_no_extraction: {prob_sentence_good_no_extraction}')
    print(f'prob_sentence_bad_extraction: {prob_sentence_bad_extraction}')
    print(f'prob_sentence_good_extraction_resumption: {prob_sentence_good_extraction_resumption}')
    print(f'prob_sentence_good_extraction_as_subject: {prob_sentence_good_extraction_as_subject}')


def main():
    print('main')

    model_name, eval_suite = arg_parse()
    model_name = 'bert-base-uncased'  # NB bert large uncased is about 1GB
    model_name = f'''models/bert-base-italian-uncased/pytorch_model.bin'''
    model_name = f'''models/bert-base-italian-cased/'''
    eval_suite = 'it'
    bert, tokenizer = init_bert_model(model_name, do_lower_case=False)
    if tokenizer is None:
        print('error, tokenizer is null')
        return

    bert_utils.check_unknown_words(tokenizer)

    sentence_to_analizse = 'Di che cosa Marco si chiede se è stata riparata da ***Luca***?'
    topk_tokens, topk_probs, topk_probs_nonsoftmax = analize_sentence(bert, tokenizer, sentence_to_analizse)
    print(f'sentence: {sentence_to_analizse}')
    print(f'topk: {topk_tokens}, top_probs: {topk_probs}, topk_probs_nonsoftmax: {topk_probs_nonsoftmax}')

    testset_data = bert_utils.load_testset_data('./outputs/syntactic_tests_it/wh_island.jsonl')
    examples_count = len(testset_data['sentences'])
    print(f'examples_count: {examples_count}')

    only_examples = [3, 6, 8, 10, 14, 15, 16, 18, 19, 21, 22, 23, 26, 29, 31, 32, 33, 39, 43, 46, 47, 48, 49]
    print(f'incorrect examples count: {len(only_examples)} out of 50 ({len(only_examples)/50})')
    for example_idx, sentence_data in enumerate(testset_data['sentences']):
        #print(f"json_str, type: {type(sentence_data)}: {sentence_data}")
        # print_sentence_pairs_probabilities(bert, tokenizer, sentence_data)
        bert_utils.analize_example(bert, tokenizer, example_idx, sentence_data)
        #return

    # run_eval(eval_suite, bert, tokenizer)
    #prob1 = estimate_sentence_probability_from_text(bert, tokenizer, 'What is your name?')
    #prob2 = estimate_sentence_probability_from_text(bert, tokenizer, 'What is name your?')
    #print(f'prob1: {prob1}, prob2: {prob2}')
    #eval_it(bert, tokenizer)
    #custom_eval("What is your name?", bert, tokenizer)


if __name__ == "__main__":
    main()


