{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1k6fQivhi3Aw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9f3c8f-2329-4c0f-c46b-43889972cbe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bert-syntax-it'...\n",
            "remote: Enumerating objects: 206, done.\u001b[K\n",
            "remote: Counting objects: 100% (206/206), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 206 (delta 93), reused 183 (delta 71), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (206/206), 3.12 MiB | 8.21 MiB/s, done.\n",
            "Resolving deltas: 100% (93/93), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ml-utils/bert-syntax-it.git\n",
        "!mv ./bert-syntax-it/marvin_linzen_dataset.tsv ./marvin_linzen_dataset.tsv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!ls -la"
      ],
      "metadata": {
        "id": "Tg8aVQDlv4hd",
        "outputId": "9ed9173a-a3b2-4175-ada3-53c440d2cdea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20244\n",
            "drwxr-xr-x 1 root root     4096 May  3 07:09 .\n",
            "drwxr-xr-x 1 root root     4096 May  3 06:44 ..\n",
            "drwxr-xr-x 6 root root     4096 May  3 07:09 bert-syntax-it\n",
            "drwxr-xr-x 4 root root     4096 Apr 29 03:18 .config\n",
            "-rw-r--r-- 1 root root 20705886 May  3 07:08 marvin_linzen_dataset.tsv\n",
            "drwxr-xr-x 1 root root     4096 Apr 29 03:19 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install folium==0.2.1\n",
        "!pip install pytorch-pretrained-bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq9Xewlm56P_",
        "outputId": "90573f0c-61c0-4759-bfc4-6b06c57f0c36"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=6a7636c4cb740c89c93f55df6ccd52db9c3e144d4ea8ce63daf6b9e6e2e22920\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n",
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.22.5-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 35.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.11.0+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.2.0)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting botocore<1.26.0,>=1.25.5\n",
            "  Downloading botocore-1.25.5-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.5->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 54.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.5->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 57.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.22.5 botocore-1.25.5 jmespath-1.0.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.2 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "\n",
        "from pytorch_pretrained_bert import BertForMaskedLM,tokenization\n",
        "import torch\n",
        "import argparse, sys\n",
        "import csv"
      ],
      "metadata": {
        "id": "uIIrJdGl6A8N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_probs_for_words(bert,tokenizer,sent,w1,w2):\n",
        "    print(f'sent: {sent}')\n",
        "    pre,target,post=sent.split('***')\n",
        "    print(f'pre: {pre}, target: {target}, post: {post}')\n",
        "    if 'mask' in target.lower():\n",
        "        target=['[MASK]']\n",
        "    else:\n",
        "        target=tokenizer.tokenize(target)\n",
        "\n",
        "    # todo, fixme: the vocabulary of the pretrained model from Kaj does not have entries for CLS, UNK\n",
        "    # fixme: tokenizer.tokenize(pre), does not recognize the words\n",
        "    tokens=['[CLS]']+tokenizer.tokenize(pre)  # tokens = tokenizer.tokenize(pre)\n",
        "\n",
        "    target_idx=len(tokens)\n",
        "\n",
        "    #print(target_idx)\n",
        "    tokens+=target+tokenizer.tokenize(post)+['[SEP]']\n",
        "    print(f'tokens {tokens}')\n",
        "    input_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "    try:\n",
        "        word_ids=tokenizer.convert_tokens_to_ids([w1,w2])\n",
        "    except KeyError:\n",
        "        print(\"skipping\",w1,w2,\"bad wins\")\n",
        "        return None\n",
        "    tens=torch.LongTensor(input_ids).unsqueeze(0)\n",
        "    res=bert(tens)[0,target_idx]\n",
        "    #res=torch.nn.functional.softmax(res,-1)\n",
        "    scores = res[word_ids]\n",
        "    return [float(x) for x in scores]\n",
        "\n",
        "from collections import Counter\n",
        "def load_marvin():\n",
        "    cc = Counter()\n",
        "    # note: I edited the LM_Syneval/src/make_templates.py script, and run \"python LM_Syneval/src/make_templates.py LM_Syneval/data/templates/ > marvin_linzen_dataset.tsv\"\n",
        "    out = []\n",
        "    for line in open(\"marvin_linzen_dataset.tsv\"):\n",
        "        case = line.strip().split(\"\\t\")\n",
        "        cc[case[1]]+=1\n",
        "        g,ug = case[-2],case[-1]\n",
        "        g = g.split()\n",
        "        ug = ug.split()\n",
        "        assert(len(g)==len(ug)),(g,ug)\n",
        "        diffs = [i for i,pair in enumerate(zip(g,ug)) if pair[0]!=pair[1]]\n",
        "        if (len(diffs)!=1):\n",
        "            #print(diffs)\n",
        "            #print(g,ug)\n",
        "            continue    \n",
        "        assert(len(diffs)==1),diffs\n",
        "        gv=g[diffs[0]]   # good\n",
        "        ugv=ug[diffs[0]] # bad\n",
        "        g[diffs[0]]=\"***mask***\"\n",
        "        g.append(\".\")\n",
        "        out.append((case[0],case[1],\" \".join(g),gv,ugv))\n",
        "    return out\n",
        "\n",
        "def eval_marvin(bert,tokenizer):\n",
        "    o = load_marvin()\n",
        "    print(len(o),file=sys.stderr)\n",
        "    from collections import defaultdict\n",
        "    import time\n",
        "    rc = defaultdict(Counter)\n",
        "    tc = Counter()\n",
        "    start = time.time()\n",
        "    for i,(case,tp,s,g,b) in enumerate(o):\n",
        "        ps = get_probs_for_words(bert,tokenizer,s,g,b)\n",
        "        if ps is None: ps = [0,1]\n",
        "        gp = ps[0]\n",
        "        bp = ps[1]\n",
        "        print(gp>bp,case,tp,g,b,s)\n",
        "        if i % 100==0:\n",
        "            print(i,time.time()-start,file=sys.stderr)\n",
        "            start=time.time()\n",
        "            sys.stdout.flush()\n",
        "\n",
        "def eval_lgd(bert,tokenizer):\n",
        "    for i,line in enumerate(open(\"lgd_dataset_with_is_are.tsv\",encoding=\"utf8\")):\n",
        "        na,_,masked,good,bad = line.strip().split(\"\\t\")\n",
        "        ps = get_probs_for_words(bert,tokenizer,masked,good,bad)\n",
        "        if ps is None: continue\n",
        "        gp = ps[0]\n",
        "        bp = ps[1]\n",
        "        print(str(gp>bp),na,good,gp,bad,bp,masked.encode(\"utf8\"),sep=u\"\\t\")\n",
        "        if i%100 == 0:\n",
        "            print(i,file=sys.stderr)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "\n",
        "def read_gulordava():\n",
        "    rows = csv.DictReader(open(\"generated.tab\",encoding=\"utf8\"),delimiter=\"\\t\")\n",
        "    data=[]\n",
        "    for row in rows:\n",
        "        row2=next(rows)\n",
        "        assert(row['sent']==row2['sent'])\n",
        "        assert(row['class']=='correct')\n",
        "        assert(row2['class']=='wrong')\n",
        "        sent = row['sent'].lower().split()[:-1] # dump the <eos> token.\n",
        "        good_form = row['form']\n",
        "        bad_form  = row2['form']\n",
        "        sent[int(row['len_prefix'])]=\"***mask***\"\n",
        "        sent = \" \".join(sent)\n",
        "        data.append((sent,row['n_attr'],good_form,bad_form))\n",
        "    return data\n",
        "\n",
        "def eval_gulordava(bert,tokenizer):\n",
        "    for i,(masked,natt,good,bad) in enumerate(read_gulordava()):\n",
        "        if good in [\"is\",\"are\"]:\n",
        "            print(\"skipping is/are\")\n",
        "            continue\n",
        "        ps = get_probs_for_words(bert,tokenizer,masked,good,bad)\n",
        "        if ps is None: continue\n",
        "        gp = ps[0]\n",
        "        bp = ps[1]\n",
        "        print(str(gp>bp),natt,good,gp,bad,bp,masked.encode(\"utf8\"),sep=u\"\\t\")\n",
        "        if i%100 == 0:\n",
        "            print(i,file=sys.stderr)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "# choose_eval()\n",
        "\n",
        "\n",
        "def init_bert_model(model_name):\n",
        "    # model_name = 'bert-large-uncased'\n",
        "    #if 'base' in sys.argv: model_name = 'bert-base-uncased'\n",
        "    print(f'model_name: {model_name}')\n",
        "    print(\"using model:\", model_name, file=sys.stderr)\n",
        "    bert = BertForMaskedLM.from_pretrained(model_name)\n",
        "    print(\"bert model loaded, getting the tokenizer..\")\n",
        "    vocab_filepath = os.path.join(model_name, 'dict.txt')\n",
        "    tokenizer = tokenization.BertTokenizer.from_pretrained(vocab_filepath)\n",
        "    print(\"tokenizer ready.\")\n",
        "\n",
        "    bert.eval()\n",
        "    return bert, tokenizer\n",
        "\n",
        "\n",
        "def run_eval(eval_suite, bert, tokenizer):\n",
        "    print('running eval..')\n",
        "    if 'marvin' == eval_suite:\n",
        "        eval_marvin(bert,tokenizer)\n",
        "    elif 'gul' == eval_suite:\n",
        "        eval_gulordava(bert,tokenizer)\n",
        "    else:\n",
        "        eval_lgd(bert,tokenizer)\n",
        "\n",
        "\n",
        "def arg_parse():\n",
        "    print('parsing args..')\n",
        "    # Python program to demonstrate\n",
        "    # command line arguments\n",
        "\n",
        "    import getopt, sys\n",
        "\n",
        "    # Remove 1st argument from the\n",
        "    # list of command line arguments\n",
        "    argumentList = sys.argv[1:]\n",
        "\n",
        "    options = \"be:\"\n",
        "\n",
        "    # Long options\n",
        "    long_options = [\"bert_model\", \"eval_suite\"]\n",
        "\n",
        "    DEFAULT_MODEL = 'bert-large-uncased'\n",
        "    DEFAULT_EVAL_SUITE = 'lgd'\n",
        "    model_name = DEFAULT_MODEL\n",
        "    eval_suite = DEFAULT_EVAL_SUITE\n",
        "\n",
        "    try:\n",
        "        # Parsing argument\n",
        "        print(f'argumentList: {argumentList}')\n",
        "\n",
        "        # checking each argument\n",
        "        for arg_idx, currentArgument  in enumerate(argumentList):\n",
        "            print(f'persing currentArgument {currentArgument}')\n",
        "            if currentArgument in (\"-h\", \"--Help\"):\n",
        "                print(\"Displaying Help\")\n",
        "\n",
        "            elif currentArgument in (\"-b\", \"--bert_model\"):\n",
        "\n",
        "                argValue = argumentList[arg_idx+1]\n",
        "                print(f'currentArgument: {currentArgument}, argValue: {argValue}')\n",
        "                if argValue == 'base':\n",
        "                    model_name = 'bert-base-uncased'\n",
        "                else:\n",
        "                    model_name = argValue\n",
        "                    print(f'set model_name: {model_name}')\n",
        "\n",
        "            elif currentArgument in (\"-e\", \"--eval_suite\"):\n",
        "                argValue = argumentList[arg_idx + 1]\n",
        "                print(f'currentArgument: {currentArgument}, argValue: {argValue}')\n",
        "                eval_suite = argValue\n",
        "\n",
        "    except getopt.error as err:\n",
        "        # output error, and return with an error code\n",
        "        print(str(err))\n",
        "\n",
        "    print(f'model_name {model_name}, eval_suite {eval_suite}')\n",
        "    return model_name, eval_suite\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('main')\n",
        "    model_name, eval_suite = arg_parse()\n",
        "    eval_suite = 'marvin'\n",
        "    bert, tokenizer = init_bert_model(model_name)\n",
        "    run_eval(eval_suite, bert, tokenizer)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Da8O3puM0hQE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "8Jsmu4GprhhY",
        "outputId": "c8b36c86-04e2-453a-e2ad-74b36bf009e0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main\n",
            "parsing args..\n",
            "argumentList: ['-f', '/root/.local/share/jupyter/runtime/kernel-3dc272cd-54fa-4625-9756-0ba678ef1978.json']\n",
            "persing currentArgument -f\n",
            "persing currentArgument /root/.local/share/jupyter/runtime/kernel-3dc272cd-54fa-4625-9756-0ba678ef1978.json\n",
            "model_name bert-large-uncased, eval_suite lgd\n",
            "model_name: bert-large-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "using model: bert-large-uncased\n",
            "Model name 'bert-large-uncased/dict.txt' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'bert-large-uncased/dict.txt' was a path or url but couldn't find any file associated to this path or url.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert model loaded, getting the tokenizer..\n",
            "tokenizer ready.\n",
            "running eval..\n",
            "sent: the author that the guard likes ***mask*** .\n",
            "pre: the author that the guard likes , target: mask, post:  .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "147506\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-6d0f16b47f58>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0meval_suite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'marvin'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_bert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0mrun_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_suite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6d0f16b47f58>\u001b[0m in \u001b[0;36mrun_eval\u001b[0;34m(eval_suite, bert, tokenizer)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running eval..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'marvin'\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0meval_suite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0meval_marvin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m'gul'\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0meval_suite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0meval_gulordava\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6d0f16b47f58>\u001b[0m in \u001b[0;36meval_marvin\u001b[0;34m(bert, tokenizer)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_probs_for_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-6d0f16b47f58>\u001b[0m in \u001b[0;36mget_probs_for_words\u001b[0;34m(bert, tokenizer, sent, w1, w2)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# todo, fixme: the vocabulary of the pretrained model from Kaj does not have entries for CLS, UNK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# fixme: tokenizer.tokenize(pre), does not recognize the words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'[CLS]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# tokens = tokenizer.tokenize(pre)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtarget_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'tokenize'"
          ]
        }
      ]
    }
  ]
}