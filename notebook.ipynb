{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1k6fQivhi3Aw",
        "outputId": "9a6af33d-6488-4b7c-e25e-677ceaba42dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bert-syntax-it'...\n",
            "remote: Enumerating objects: 194, done.\u001b[K\n",
            "remote: Counting objects: 100% (194/194), done.\u001b[K\n",
            "remote: Compressing objects: 100% (118/118), done.\u001b[K\n",
            "remote: Total 194 (delta 88), reused 173 (delta 69), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (194/194), 133.27 KiB | 2.61 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ml-utils/bert-syntax-it.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install folium==0.2.1\n",
        "!pip install pytorch-pretrained-bert"
      ],
      "metadata": {
        "id": "Zq9Xewlm56P_",
        "outputId": "24a90669-4c2b-4d39-8084-70cee6c38fde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 20 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 30 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 40 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 61 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 69 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=e467fbd0cfaaa9ab743d1a46a028c2158e425dbfd0a1b37d5616f7dfa7a2a566\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.12.1.post1\n",
            "    Uninstalling folium-0.12.1.post1:\n",
            "      Successfully uninstalled folium-0.12.1.post1\n",
            "Successfully installed folium-0.2.1\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.11.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.22.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.2.0)\n",
            "Requirement already satisfied: botocore<1.26.0,>=1.25.5 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.25.5)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.0.0)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.5.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.5->boto3->pytorch-pretrained-bert) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.5->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.5->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "\n",
        "from pytorch_pretrained_bert import BertForMaskedLM,tokenization\n",
        "import torch\n",
        "import argparse, sys\n",
        "import csv"
      ],
      "metadata": {
        "id": "uIIrJdGl6A8N"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def get_probs_for_words(bert,tokenizer,sent,w1,w2):\n",
        "    print(f'sent: {sent}')\n",
        "    pre,target,post=sent.split('***')\n",
        "    print(f'pre: {pre}, target: {target}, post: {post}')\n",
        "    if 'mask' in target.lower():\n",
        "        target=['[MASK]']\n",
        "    else:\n",
        "        target=tokenizer.tokenize(target)\n",
        "\n",
        "    # todo, fixme: the vocabulary of the pretrained model from Kaj does not have entries for CLS, UNK\n",
        "    # fixme: tokenizer.tokenize(pre), does not recognize the words\n",
        "    tokens=['[CLS]']+tokenizer.tokenize(pre)  # tokens = tokenizer.tokenize(pre)\n",
        "\n",
        "    target_idx=len(tokens)\n",
        "\n",
        "    #print(target_idx)\n",
        "    tokens+=target+tokenizer.tokenize(post)+['[SEP]']\n",
        "    print(f'tokens {tokens}')\n",
        "    input_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "    try:\n",
        "        word_ids=tokenizer.convert_tokens_to_ids([w1,w2])\n",
        "    except KeyError:\n",
        "        print(\"skipping\",w1,w2,\"bad wins\")\n",
        "        return None\n",
        "    tens=torch.LongTensor(input_ids).unsqueeze(0)\n",
        "    res=bert(tens)[0,target_idx]\n",
        "    #res=torch.nn.functional.softmax(res,-1)\n",
        "    scores = res[word_ids]\n",
        "    return [float(x) for x in scores]\n",
        "\n",
        "from collections import Counter\n",
        "def load_marvin():\n",
        "    cc = Counter()\n",
        "    # note: I edited the LM_Syneval/src/make_templates.py script, and run \"python LM_Syneval/src/make_templates.py LM_Syneval/data/templates/ > marvin_linzen_dataset.tsv\"\n",
        "    out = []\n",
        "    for line in open(\"marvin_linzen_dataset.tsv\"):\n",
        "        case = line.strip().split(\"\\t\")\n",
        "        cc[case[1]]+=1\n",
        "        g,ug = case[-2],case[-1]\n",
        "        g = g.split()\n",
        "        ug = ug.split()\n",
        "        assert(len(g)==len(ug)),(g,ug)\n",
        "        diffs = [i for i,pair in enumerate(zip(g,ug)) if pair[0]!=pair[1]]\n",
        "        if (len(diffs)!=1):\n",
        "            #print(diffs)\n",
        "            #print(g,ug)\n",
        "            continue    \n",
        "        assert(len(diffs)==1),diffs\n",
        "        gv=g[diffs[0]]   # good\n",
        "        ugv=ug[diffs[0]] # bad\n",
        "        g[diffs[0]]=\"***mask***\"\n",
        "        g.append(\".\")\n",
        "        out.append((case[0],case[1],\" \".join(g),gv,ugv))\n",
        "    return out\n",
        "\n",
        "def eval_marvin(bert,tokenizer):\n",
        "    o = load_marvin()\n",
        "    print(len(o),file=sys.stderr)\n",
        "    from collections import defaultdict\n",
        "    import time\n",
        "    rc = defaultdict(Counter)\n",
        "    tc = Counter()\n",
        "    start = time.time()\n",
        "    for i,(case,tp,s,g,b) in enumerate(o):\n",
        "        ps = get_probs_for_words(bert,tokenizer,s,g,b)\n",
        "        if ps is None: ps = [0,1]\n",
        "        gp = ps[0]\n",
        "        bp = ps[1]\n",
        "        print(gp>bp,case,tp,g,b,s)\n",
        "        if i % 100==0:\n",
        "            print(i,time.time()-start,file=sys.stderr)\n",
        "            start=time.time()\n",
        "            sys.stdout.flush()\n",
        "\n",
        "def eval_lgd(bert,tokenizer):\n",
        "    for i,line in enumerate(open(\"lgd_dataset_with_is_are.tsv\",encoding=\"utf8\")):\n",
        "        na,_,masked,good,bad = line.strip().split(\"\\t\")\n",
        "        ps = get_probs_for_words(bert,tokenizer,masked,good,bad)\n",
        "        if ps is None: continue\n",
        "        gp = ps[0]\n",
        "        bp = ps[1]\n",
        "        print(str(gp>bp),na,good,gp,bad,bp,masked.encode(\"utf8\"),sep=u\"\\t\")\n",
        "        if i%100 == 0:\n",
        "            print(i,file=sys.stderr)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "\n",
        "def read_gulordava():\n",
        "    rows = csv.DictReader(open(\"generated.tab\",encoding=\"utf8\"),delimiter=\"\\t\")\n",
        "    data=[]\n",
        "    for row in rows:\n",
        "        row2=next(rows)\n",
        "        assert(row['sent']==row2['sent'])\n",
        "        assert(row['class']=='correct')\n",
        "        assert(row2['class']=='wrong')\n",
        "        sent = row['sent'].lower().split()[:-1] # dump the <eos> token.\n",
        "        good_form = row['form']\n",
        "        bad_form  = row2['form']\n",
        "        sent[int(row['len_prefix'])]=\"***mask***\"\n",
        "        sent = \" \".join(sent)\n",
        "        data.append((sent,row['n_attr'],good_form,bad_form))\n",
        "    return data\n",
        "\n",
        "def eval_gulordava(bert,tokenizer):\n",
        "    for i,(masked,natt,good,bad) in enumerate(read_gulordava()):\n",
        "        if good in [\"is\",\"are\"]:\n",
        "            print(\"skipping is/are\")\n",
        "            continue\n",
        "        ps = get_probs_for_words(bert,tokenizer,masked,good,bad)\n",
        "        if ps is None: continue\n",
        "        gp = ps[0]\n",
        "        bp = ps[1]\n",
        "        print(str(gp>bp),natt,good,gp,bad,bp,masked.encode(\"utf8\"),sep=u\"\\t\")\n",
        "        if i%100 == 0:\n",
        "            print(i,file=sys.stderr)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "# choose_eval()\n",
        "\n",
        "\n",
        "def init_bert_model(model_name):\n",
        "    # model_name = 'bert-large-uncased'\n",
        "    #if 'base' in sys.argv: model_name = 'bert-base-uncased'\n",
        "    print(f'model_name: {model_name}')\n",
        "    print(\"using model:\", model_name, file=sys.stderr)\n",
        "    bert = BertForMaskedLM.from_pretrained(model_name)\n",
        "    print(\"bert model loaded, getting the tokenizer..\")\n",
        "    vocab_filepath = os.path.join(model_name, 'dict.txt')\n",
        "    tokenizer = tokenization.BertTokenizer.from_pretrained(vocab_filepath)\n",
        "    print(\"tokenizer ready.\")\n",
        "\n",
        "    bert.eval()\n",
        "    return bert, tokenizer\n",
        "\n",
        "\n",
        "def run_eval(eval_suite, bert, tokenizer):\n",
        "    print('running eval..')\n",
        "    if 'marvin' == eval_suite:\n",
        "        eval_marvin(bert,tokenizer)\n",
        "    elif 'gul' == eval_suite:\n",
        "        eval_gulordava(bert,tokenizer)\n",
        "    else:\n",
        "        eval_lgd(bert,tokenizer)\n",
        "\n",
        "\n",
        "def arg_parse():\n",
        "    print('parsing args..')\n",
        "    # Python program to demonstrate\n",
        "    # command line arguments\n",
        "\n",
        "    import getopt, sys\n",
        "\n",
        "    # Remove 1st argument from the\n",
        "    # list of command line arguments\n",
        "    argumentList = sys.argv[1:]\n",
        "\n",
        "    options = \"be:\"\n",
        "\n",
        "    # Long options\n",
        "    long_options = [\"bert_model\", \"eval_suite\"]\n",
        "\n",
        "    DEFAULT_MODEL = 'bert-large-uncased'\n",
        "    DEFAULT_EVAL_SUITE = 'lgd'\n",
        "    model_name = DEFAULT_MODEL\n",
        "    eval_suite = DEFAULT_EVAL_SUITE\n",
        "\n",
        "    try:\n",
        "        # Parsing argument\n",
        "        print(f'argumentList: {argumentList}')\n",
        "\n",
        "        # checking each argument\n",
        "        for arg_idx, currentArgument  in enumerate(argumentList):\n",
        "            print(f'persing currentArgument {currentArgument}')\n",
        "            if currentArgument in (\"-h\", \"--Help\"):\n",
        "                print(\"Displaying Help\")\n",
        "\n",
        "            elif currentArgument in (\"-b\", \"--bert_model\"):\n",
        "\n",
        "                argValue = argumentList[arg_idx+1]\n",
        "                print(f'currentArgument: {currentArgument}, argValue: {argValue}')\n",
        "                if argValue == 'base':\n",
        "                    model_name = 'bert-base-uncased'\n",
        "                else:\n",
        "                    model_name = argValue\n",
        "                    print(f'set model_name: {model_name}')\n",
        "\n",
        "            elif currentArgument in (\"-e\", \"--eval_suite\"):\n",
        "                argValue = argumentList[arg_idx + 1]\n",
        "                print(f'currentArgument: {currentArgument}, argValue: {argValue}')\n",
        "                eval_suite = argValue\n",
        "\n",
        "    except getopt.error as err:\n",
        "        # output error, and return with an error code\n",
        "        print(str(err))\n",
        "\n",
        "    print(f'model_name {model_name}, eval_suite {eval_suite}')\n",
        "    return model_name, eval_suite\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('main')\n",
        "    model_name, eval_suite = arg_parse()\n",
        "\n",
        "    bert, tokenizer = init_bert_model(model_name)\n",
        "    run_eval(eval_suite, bert, tokenizer)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "main()"
      ],
      "metadata": {
        "id": "Da8O3puM0hQE",
        "outputId": "ca613634-c443-4c96-c97c-8e906845c5e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main\n",
            "parsing args..\n",
            "argumentList: ['-f', '/root/.local/share/jupyter/runtime/kernel-d9b0c4b2-4468-4211-8c71-2aebc7265ece.json']\n",
            "persing currentArgument -f\n",
            "persing currentArgument /root/.local/share/jupyter/runtime/kernel-d9b0c4b2-4468-4211-8c71-2aebc7265ece.json\n",
            "model_name bert-large-uncased, eval_suite lgd\n",
            "model_name: bert-large-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "using model: bert-large-uncased\n",
            "100%|██████████| 1248501532/1248501532 [00:31<00:00, 39529338.26B/s]\n",
            "Model name 'bert-large-uncased/dict.txt' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'bert-large-uncased/dict.txt' was a path or url but couldn't find any file associated to this path or url.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert model loaded, getting the tokenizer..\n",
            "tokenizer ready.\n",
            "running eval..\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4fbeb2cc73e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4fbeb2cc73e4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_bert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mrun_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_suite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4fbeb2cc73e4>\u001b[0m in \u001b[0;36mrun_eval\u001b[0;34m(eval_suite, bert, tokenizer)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0meval_gulordava\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0meval_lgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4fbeb2cc73e4>\u001b[0m in \u001b[0;36meval_lgd\u001b[0;34m(bert, tokenizer)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_lgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lgd_dataset_with_is_are.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mna\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgood\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_probs_for_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgood\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lgd_dataset_with_is_are.tsv'"
          ]
        }
      ]
    }
  ]
}