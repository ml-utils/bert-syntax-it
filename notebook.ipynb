{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1k6fQivhi3Aw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53460d2b-ed92-44cf-85dc-ecdf11f1671e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bert-syntax-it'...\n",
            "remote: Enumerating objects: 209, done.\u001b[K\n",
            "remote: Counting objects: 100% (209/209), done.\u001b[K\n",
            "remote: Compressing objects: 100% (129/129), done.\u001b[K\n",
            "remote: Total 209 (delta 95), reused 185 (delta 72), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (209/209), 3.12 MiB | 9.40 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ml-utils/bert-syntax-it.git\n",
        "!mv ./bert-syntax-it/marvin_linzen_dataset.tsv ./marvin_linzen_dataset.tsv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg8aVQDlv4hd",
        "outputId": "d317e46e-34dd-4e32-ee4e-76f37846009b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20244\n",
            "drwxr-xr-x 1 root root     4096 May  4 08:19 .\n",
            "drwxr-xr-x 1 root root     4096 May  4 08:17 ..\n",
            "drwxr-xr-x 6 root root     4096 May  4 08:19 bert-syntax-it\n",
            "drwxr-xr-x 4 root root     4096 Apr 29 03:18 .config\n",
            "-rw-r--r-- 1 root root 20705886 May  4 08:19 marvin_linzen_dataset.tsv\n",
            "drwxr-xr-x 1 root root     4096 Apr 29 03:19 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install folium==0.2.1\n",
        "!pip install pytorch-pretrained-bert\n",
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq9Xewlm56P_",
        "outputId": "363c1010-7773-46e4-a5c5-75b8d4d678a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 24.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 20 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 30 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 40 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 51 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 69 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=13cc90c3e878cc1c3032e22e1c8eae5b65956e7458eb47f2c9ed8292db14794c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n",
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.22.7-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 39.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.2.0)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting botocore<1.26.0,>=1.25.7\n",
            "  Downloading botocore-1.25.7-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 47.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.26.0,>=1.25.7->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.7->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.10.8)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.22.7 botocore-1.25.7 jmespath-1.0.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.2 urllib3-1.25.11\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 32.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 32.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 35.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=6901c64b53794e753e3641d0e5388be1e058abc4eb2e2c6db886492d84e7f470\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "from collections import Counter\n",
        "\n",
        "from pytorch_pretrained_bert import BertForMaskedLM,tokenization\n",
        "from torch.nn.functional import softmax\n",
        "import torch\n",
        "import argparse, sys\n",
        "import csv"
      ],
      "metadata": {
        "id": "uIIrJdGl6A8N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_sentence(tokenizer,sent):\n",
        "    print(f'sent: {sent}')\n",
        "    pre,target,post=sent.split('***')\n",
        "    print(f'pre: {pre}, target: {target}, post: {post}')\n",
        "    if 'mask' in target.lower():\n",
        "        target=['[MASK]']\n",
        "    else:\n",
        "        target=tokenizer.tokenize(target)\n",
        "\n",
        "    # todo, fixme: the vocabulary of the pretrained model from Kaj does not have entries for CLS, UNK\n",
        "    # fixme: tokenizer.tokenize(pre), does not recognize the words\n",
        "    tokens=['[CLS]']+tokenizer.tokenize(pre)  # tokens = tokenizer.tokenize(pre)\n",
        "    target_idx=len(tokens)\n",
        "    print(f'target_idx: {target_idx}')\n",
        "    tokens+=target+tokenizer.tokenize(post)+['[SEP]']\n",
        "    print(f'tokens {tokens}')\n",
        "    return tokens, target_idx\n",
        "\n",
        "\n",
        "def get_sentence_probs_from_word_ids(bert, tokenizer, sentence_ids, masked_words_ids, masked_word_idx):\n",
        "    tens=torch.LongTensor(sentence_ids).unsqueeze(0)\n",
        "\n",
        "    res_unsliced = bert(tens)\n",
        "    res=res_unsliced[0, masked_word_idx]\n",
        "\n",
        "    # todo: produce probabilities not with softmax (not using an exponential, to avoiding the maximization of top results),\n",
        "    #  then compare these probailities with the softmax ones, expecially for ungrammatical sentences\n",
        "    res_softmax = softmax(res.detach(), -1)\n",
        "    #RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
        "\n",
        "    topk_probs, topk_ids = torch.topk(res_softmax, 10)  # todo: get ids/indexes, not their probability value\n",
        "    topk_tokens = convert_ids_to_tokens(tokenizer, topk_ids)\n",
        "\n",
        "    scores = res[masked_words_ids]\n",
        "    probs = [float(x) for x in scores]\n",
        "    scores_softmax = res_softmax[masked_words_ids]\n",
        "    probs_softmax = [float(x) for x in scores_softmax]\n",
        "\n",
        "    return probs_softmax  # probs\n",
        "\n",
        "\n",
        "def estimate_sentence_probability_from_text(bert, tokenizer, sentence):\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    sentence_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    return estimate_sentence_probability(bert, tokenizer, sentence_ids)\n",
        "\n",
        "\n",
        "def estimate_sentence_probability(bert, tokenizer, sentence_ids):\n",
        "    # iterate for each word, mask it and get the probability\n",
        "    # sum the logs of the probabilities\n",
        "\n",
        "    MASK_ID = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
        "    CLS_ID = tokenizer.convert_tokens_to_ids(['[CLS]'])[0]\n",
        "    SEP_ID = tokenizer.convert_tokens_to_ids(['[SEP]'])[0]\n",
        "\n",
        "    sentence_prob_estimate = 0\n",
        "    for masked_index in range(len(sentence_ids)):\n",
        "        masked_sentence_ids = sentence_ids.copy()\n",
        "        masked_word_id = masked_sentence_ids[masked_index]\n",
        "        masked_sentence_ids[masked_index] = MASK_ID\n",
        "        masked_sentence_ids = [CLS_ID] + masked_sentence_ids + [SEP_ID]\n",
        "        print(f'testing {convert_ids_to_tokens(tokenizer, [masked_word_id])} '\n",
        "              f'at {masked_index+1} in sentence {convert_ids_to_tokens(tokenizer, masked_sentence_ids)}')\n",
        "        probability_of_this_masking = get_sentence_probs_from_word_ids(bert, tokenizer, masked_sentence_ids,\n",
        "                                                                     [masked_word_id], masked_index+1)\n",
        "        sentence_prob_estimate += np.log(probability_of_this_masking[0])\n",
        "\n",
        "    # todo: also alternative method with formula from paper balanced on sentence lenght\n",
        "\n",
        "    return np.exp(sentence_prob_estimate)\n",
        "\n",
        "\n",
        "def get_probs_for_words(bert, tokenizer, sent, w1, w2):\n",
        "    tokens, masked_word_idx = tokenize_sentence(tokenizer, sent)\n",
        "\n",
        "    sentence_ids=tokenizer.convert_tokens_to_ids(tokens)\n",
        "    try:\n",
        "        masked_words_ids=tokenizer.convert_tokens_to_ids([w1,w2])\n",
        "    except KeyError:\n",
        "        print(\"skipping\",w1,w2,\"bad wins\")\n",
        "        return None\n",
        "\n",
        "    return get_sentence_probs_from_word_ids(bert, tokenizer, sentence_ids, masked_words_ids, masked_word_idx)\n",
        "\n",
        "\n",
        "def convert_ids_to_tokens(tokenizer, ids):\n",
        "    \"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"\n",
        "    tokens = []\n",
        "    for i in ids:\n",
        "        if torch.is_tensor(i):\n",
        "            i = i.item()\n",
        "        # print(f\"id: {i}, type: {type(i)}\")\n",
        "        try:\n",
        "            tokens.append(tokenizer.ids_to_tokens[i])\n",
        "        except:\n",
        "            print(f\"Unable to find id {i} {type(i)} in the vocabulary\")\n",
        "\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def load_it():\n",
        "    cc = Counter()\n",
        "    # note: I edited the LM_Syneval/src/make_templates.py script, and run \"python LM_Syneval/src/make_templates.py LM_Syneval/data/templates/ > marvin_linzen_dataset.tsv\"\n",
        "    out = []\n",
        "    for line in open(\"it_dataset.tsv\"):\n",
        "        case = line.strip().split(\"\\t\")\n",
        "        cc[case[1]]+=1\n",
        "        g,ug = case[-2],case[-1]\n",
        "        g = g.split()\n",
        "        ug = ug.split()\n",
        "        assert(len(g)==len(ug)),(g,ug)\n",
        "        diffs = [i for i,pair in enumerate(zip(g,ug)) if pair[0]!=pair[1]]\n",
        "        if (len(diffs)!=1):\n",
        "            #print(diffs)\n",
        "            #print(g,ug)\n",
        "            continue\n",
        "        assert(len(diffs)==1),diffs\n",
        "        gv=g[diffs[0]]   # good\n",
        "        ugv=ug[diffs[0]] # bad\n",
        "        g[diffs[0]]=\"***mask***\"\n",
        "        g.append(\".\")\n",
        "        out.append((case[0],case[1],\" \".join(g),gv,ugv))\n",
        "    return out\n",
        "\n",
        "def load_marvin():\n",
        "    cc = Counter()\n",
        "    # note: I edited the LM_Syneval/src/make_templates.py script, and run \"python LM_Syneval/src/make_templates.py LM_Syneval/data/templates/ > marvin_linzen_dataset.tsv\"\n",
        "    out = []\n",
        "    for line in open(\"marvin_linzen_dataset.tsv\"):\n",
        "        case = line.strip().split(\"\\t\")\n",
        "        cc[case[1]]+=1\n",
        "        g,ug = case[-2],case[-1]\n",
        "        g = g.split()\n",
        "        ug = ug.split()\n",
        "        assert(len(g)==len(ug)),(g,ug)\n",
        "        diffs = [i for i,pair in enumerate(zip(g,ug)) if pair[0]!=pair[1]]\n",
        "        if (len(diffs)!=1):\n",
        "            #print(diffs)\n",
        "            #print(g,ug)\n",
        "            continue    \n",
        "        assert(len(diffs)==1),diffs\n",
        "        gv=g[diffs[0]]   # good\n",
        "        ugv=ug[diffs[0]] # bad\n",
        "        g[diffs[0]]=\"***mask***\"\n",
        "        g.append(\".\")\n",
        "        out.append((case[0],case[1],\" \".join(g),gv,ugv))\n",
        "    return out\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKCYAN = '\\033[96m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n",
        "\n",
        "def eval_it(bert,tokenizer):\n",
        "    o = load_it()\n",
        "    print(len(o), file=sys.stderr)\n",
        "    from collections import defaultdict\n",
        "    import time\n",
        "    rc = defaultdict(Counter)\n",
        "    tc = Counter()\n",
        "    start = time.time()\n",
        "    print(f'{bcolors.WARNING}{len(o)} sentences to process..{bcolors.ENDC}')\n",
        "    for i, (case, tp, s, good_word, bad_word) in enumerate(o):\n",
        "        ps = get_probs_for_words(bert, tokenizer, s, good_word, bad_word)\n",
        "        if ps is None: ps = [0, 1]\n",
        "        gp = ps[0]\n",
        "        bp = ps[1]\n",
        "        print(gp > bp, case, tp, good_word, bad_word, s)\n",
        "        if i % 100 == 0:\n",
        "            print(f'{bcolors.WARNING}{i}{bcolors.ENDC}')\n",
        "            print(i, time.time() - start, file=sys.stderr)\n",
        "            start = time.time()\n",
        "            sys.stdout.flush()\n",
        "\n",
        "def eval_marvin(bert,tokenizer):\n",
        "    o = load_marvin()\n",
        "    print(len(o),file=sys.stderr)\n",
        "    from collections import defaultdict\n",
        "    import time\n",
        "    rc = defaultdict(Counter)\n",
        "    tc = Counter()\n",
        "    start = time.time()\n",
        "    for i,(case,tp,s,g,b) in enumerate(o):\n",
        "        ps = get_probs_for_words(bert,tokenizer,s,g,b)\n",
        "        if ps is None: ps = [0,1]\n",
        "        gp = ps[0]\n",
        "        bp = ps[1]\n",
        "        print(gp>bp,case,tp,g,b,s)\n",
        "        if i % 100==0:\n",
        "            print(i,time.time()-start,file=sys.stderr)\n",
        "            start=time.time()\n",
        "            sys.stdout.flush()\n",
        "\n",
        "def eval_lgd(bert,tokenizer):\n",
        "    for i,line in enumerate(open(\"lgd_dataset_with_is_are.tsv\",encoding=\"utf8\")):\n",
        "        na,_,masked,good,bad = line.strip().split(\"\\t\")\n",
        "        ps = get_probs_for_words(bert,tokenizer,masked,good,bad)\n",
        "        if ps is None: continue\n",
        "        gp = ps[0]\n",
        "        bp = ps[1]\n",
        "        print(str(gp>bp),na,good,gp,bad,bp,masked.encode(\"utf8\"),sep=u\"\\t\")\n",
        "        if i%100 == 0:\n",
        "            print(i,file=sys.stderr)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "\n",
        "def read_gulordava():\n",
        "    rows = csv.DictReader(open(\"generated.tab\",encoding=\"utf8\"),delimiter=\"\\t\")\n",
        "    data=[]\n",
        "    for row in rows:\n",
        "        row2=next(rows)\n",
        "        assert(row['sent']==row2['sent'])\n",
        "        assert(row['class']=='correct')\n",
        "        assert(row2['class']=='wrong')\n",
        "        sent = row['sent'].lower().split()[:-1] # dump the <eos> token.\n",
        "        good_form = row['form']\n",
        "        bad_form  = row2['form']\n",
        "        sent[int(row['len_prefix'])]=\"***mask***\"\n",
        "        sent = \" \".join(sent)\n",
        "        data.append((sent,row['n_attr'],good_form,bad_form))\n",
        "    return data\n",
        "\n",
        "\n",
        "def eval_gulordava(bert,tokenizer):\n",
        "    for i,(masked,natt,good,bad) in enumerate(read_gulordava()):\n",
        "        if good in [\"is\",\"are\"]:\n",
        "            print(\"skipping is/are\")\n",
        "            continue\n",
        "        ps = get_probs_for_words(bert,tokenizer,masked,good,bad)\n",
        "        if ps is None: continue\n",
        "        gp = ps[0]\n",
        "        bp = ps[1]\n",
        "        print(str(gp>bp),natt,good,gp,bad,bp,masked.encode(\"utf8\"),sep=u\"\\t\")\n",
        "        if i%100 == 0:\n",
        "            print(i,file=sys.stderr)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "# choose_eval()\n",
        "\n",
        "\n",
        "def init_bert_model(model_name, dict_name=None):\n",
        "    # model_name = 'bert-large-uncased'\n",
        "    #if 'base' in sys.argv: model_name = 'bert-base-uncased'\n",
        "    print(f'model_name: {model_name}')\n",
        "    print(\"loading model:\", model_name, file=sys.stderr)\n",
        "    bert = BertForMaskedLM.from_pretrained(model_name)\n",
        "    print(\"bert model loaded, getting the tokenizer..\")\n",
        "\n",
        "    if dict_name is None:\n",
        "        vocab_filepath = model_name\n",
        "    else:\n",
        "        vocab_filepath = os.path.join(model_name, 'dict.txt')\n",
        "    tokenizer = tokenization.BertTokenizer.from_pretrained(vocab_filepath)\n",
        "\n",
        "    print(\"tokenizer ready.\")\n",
        "\n",
        "    bert.eval()\n",
        "    return bert, tokenizer\n",
        "\n",
        "\n",
        "def run_eval(eval_suite, bert, tokenizer):\n",
        "    print(f'running eval, eval_suite: {eval_suite}')\n",
        "    if 'it' == eval_suite:\n",
        "        eval_it(bert, tokenizer)\n",
        "    elif 'marvin' == eval_suite:\n",
        "        eval_marvin(bert,tokenizer)\n",
        "    elif 'gul' == eval_suite:\n",
        "        eval_gulordava(bert,tokenizer)\n",
        "    else:\n",
        "        eval_lgd(bert,tokenizer)\n",
        "\n",
        "\n",
        "def arg_parse():\n",
        "    print('parsing args..')\n",
        "    # Python program to demonstrate\n",
        "    # command line arguments\n",
        "\n",
        "    import getopt, sys\n",
        "\n",
        "    # Remove 1st argument from the\n",
        "    # list of command line arguments\n",
        "    argumentList = sys.argv[1:]\n",
        "\n",
        "    options = \"be:\"\n",
        "\n",
        "    # Long options\n",
        "    long_options = [\"bert_model\", \"eval_suite\"]\n",
        "\n",
        "    DEFAULT_MODEL = 'bert-large-uncased'\n",
        "    DEFAULT_EVAL_SUITE = 'lgd'\n",
        "    model_name = DEFAULT_MODEL\n",
        "    eval_suite = DEFAULT_EVAL_SUITE\n",
        "\n",
        "    try:\n",
        "        # Parsing argument\n",
        "        print(f'argumentList: {argumentList}')\n",
        "\n",
        "        # checking each argument\n",
        "        for arg_idx, currentArgument  in enumerate(argumentList):\n",
        "            print(f'persing currentArgument {currentArgument}')\n",
        "            if currentArgument in (\"-h\", \"--Help\"):\n",
        "                print(\"Displaying Help\")\n",
        "\n",
        "            elif currentArgument in (\"-b\", \"--bert_model\"):\n",
        "\n",
        "                argValue = argumentList[arg_idx+1]\n",
        "                print(f'currentArgument: {currentArgument}, argValue: {argValue}')\n",
        "                if argValue == 'base':\n",
        "                    model_name = 'bert-base-uncased'\n",
        "                else:\n",
        "                    model_name = argValue\n",
        "                    print(f'set model_name: {model_name}')\n",
        "\n",
        "            elif currentArgument in (\"-e\", \"--eval_suite\"):\n",
        "                argValue = argumentList[arg_idx + 1]\n",
        "                print(f'currentArgument: {currentArgument}, argValue: {argValue}')\n",
        "                eval_suite = argValue\n",
        "\n",
        "    except getopt.error as err:\n",
        "        # output error, and return with an error code\n",
        "        print(str(err))\n",
        "\n",
        "    print(f'model_name {model_name}, eval_suite {eval_suite}')\n",
        "    return model_name, eval_suite\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "def bert_get_logprobs(tokenize_input, tokenize_context, model, tokenizer, device, use_context=False):\n",
        "    batched_indexed_tokens = []\n",
        "    batched_segment_ids = []\n",
        "\n",
        "    if not use_context:\n",
        "        tokenize_combined = [\"[CLS]\"] + tokenize_input + [\"[SEP]\"]\n",
        "    else:\n",
        "        tokenize_combined = [\"[CLS]\"] + tokenize_context + tokenize_input + [\"[SEP]\"]\n",
        "\n",
        "    for i in range(len(tokenize_input)):\n",
        "        # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "        masked_index = i + 1 + (len(tokenize_context) if use_context else 0)\n",
        "        tokenize_masked = tokenize_combined.copy()\n",
        "        tokenize_masked[masked_index] = '[MASK]'\n",
        "        # unidir bert\n",
        "        # for j in range(masked_index, len(tokenize_combined)-1):\n",
        "        #    tokenize_masked[j] = '[MASK]'\n",
        "\n",
        "        # Convert token to vocabulary indices\n",
        "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenize_masked)\n",
        "        # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "        segment_ids = [0] * len(tokenize_masked)\n",
        "\n",
        "        batched_indexed_tokens.append(indexed_tokens)\n",
        "        batched_segment_ids.append(segment_ids)\n",
        "\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    tokens_tensor = torch.tensor(batched_indexed_tokens, device=device)\n",
        "    segment_tensor = torch.tensor(batched_segment_ids, device=device)\n",
        "\n",
        "    # Predict all tokens\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, token_type_ids=segment_tensor)\n",
        "        predictions = outputs[0]\n",
        "\n",
        "    # go through each word and sum their logprobs\n",
        "    lp = 0.0\n",
        "    for i in range(len(tokenize_input)):\n",
        "        masked_index = i + 1 + (len(tokenize_context) if use_context else 0)\n",
        "        predicted_score = predictions[i, masked_index]\n",
        "        predicted_prob = softmax(predicted_score.cpu().numpy())\n",
        "        lp += np.log(predicted_prob[tokenizer.convert_tokens_to_ids([tokenize_combined[masked_index]])[0]])\n",
        "\n",
        "    return lp\n",
        "\n",
        "\n",
        "def get_masked_word_probability(bert, tokenizer):\n",
        "    return 0\n",
        "\n",
        "\n",
        "def custom_eval(sentence, bert, tokenizer):\n",
        "    bert, tokenizer = init_bert_model('bert-base-uncased')\n",
        "\n",
        "    compare_tokens, compare_target_idx = tokenize_sentence(tokenizer, \"What is ***your*** name?\")\n",
        "\n",
        "    bare_sentence_tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "    paper_logprobs = bert_get_logprobs(bare_sentence_tokens, None, bert, tokenizer, device=None)\n",
        "\n",
        "    tokens_list = ['[CLS]'] + bare_sentence_tokens + ['[SEP]']\n",
        "    target_idx = 3\n",
        "    masked_word = tokens_list[target_idx]\n",
        "    tokens_list[target_idx] = '[MASK]'\n",
        "\n",
        "    print(f'tokens: {tokens_list}, masked_word: {masked_word}')\n",
        "    print(f'compare_tokens: {compare_tokens}')\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens_list)\n",
        "\n",
        "    try:\n",
        "        masked_word_id = tokenizer.convert_tokens_to_ids([masked_word])\n",
        "    except KeyError:\n",
        "        print(f\"unable to convert {masked_word} to id\")\n",
        "        return None\n",
        "    tens = torch.LongTensor(input_ids).unsqueeze(0)\n",
        "\n",
        "    res_unsliced = bert(tens)\n",
        "    res=res_unsliced[0, target_idx]\n",
        "\n",
        "    # res=torch.nn.functional.softmax(res,-1)\n",
        "\n",
        "    pred = bert(\"What is [MASK] name?\")\n",
        "\n",
        "    # Set the maximum sequence length.\n",
        "    MAX_LEN = 128\n",
        "    # Pad our input tokens\n",
        "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                              maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('main')\n",
        "    model_name, eval_suite = arg_parse()\n",
        "    model_name = 'bert-base-uncased'  # NB bert large uncased is about 1GB\n",
        "    eval_suite = 'it'\n",
        "    bert, tokenizer = init_bert_model(model_name)\n",
        "    if tokenizer is None:\n",
        "        print('error, tokenizer is null')\n",
        "        return\n",
        "\n",
        "    # run_eval(eval_suite, bert, tokenizer)\n",
        "    prob1 = estimate_sentence_probability_from_text(bert, tokenizer, 'What is your name?')\n",
        "    prob2 = estimate_sentence_probability_from_text(bert, tokenizer, 'What is name your?')\n",
        "    print(f'prob1: {prob1}, prob2: {prob2}')\n",
        "    #eval_it(bert, tokenizer)\n",
        "    #custom_eval(\"What is your name?\", bert, tokenizer)\n"
      ],
      "metadata": {
        "id": "Da8O3puM0hQE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Jsmu4GprhhY",
        "outputId": "5e2354b6-5353-4f13-99cd-c07c4a693bee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main\n",
            "parsing args..\n",
            "argumentList: ['-f', '/root/.local/share/jupyter/runtime/kernel-5dfbee09-a781-45ea-9ccc-a931b3177835.json']\n",
            "persing currentArgument -f\n",
            "persing currentArgument /root/.local/share/jupyter/runtime/kernel-5dfbee09-a781-45ea-9ccc-a931b3177835.json\n",
            "model_name bert-large-uncased, eval_suite lgd\n",
            "model_name: bert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading model: bert-base-uncased\n",
            "100%|██████████| 407873900/407873900 [00:17<00:00, 22690590.47B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert model loaded, getting the tokenizer..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1978723.31B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer ready.\n",
            "testing ['what'] at 1 in sentence ['[CLS]', '[MASK]', 'is', 'your', 'name', '?', '[SEP]']\n",
            "testing ['is'] at 2 in sentence ['[CLS]', 'what', '[MASK]', 'your', 'name', '?', '[SEP]']\n",
            "testing ['your'] at 3 in sentence ['[CLS]', 'what', 'is', '[MASK]', 'name', '?', '[SEP]']\n",
            "testing ['name'] at 4 in sentence ['[CLS]', 'what', 'is', 'your', '[MASK]', '?', '[SEP]']\n",
            "testing ['?'] at 5 in sentence ['[CLS]', 'what', 'is', 'your', 'name', '[MASK]', '[SEP]']\n",
            "testing ['what'] at 1 in sentence ['[CLS]', '[MASK]', 'is', 'name', 'your', '?', '[SEP]']\n",
            "testing ['is'] at 2 in sentence ['[CLS]', 'what', '[MASK]', 'name', 'your', '?', '[SEP]']\n",
            "testing ['name'] at 3 in sentence ['[CLS]', 'what', 'is', '[MASK]', 'your', '?', '[SEP]']\n",
            "testing ['your'] at 4 in sentence ['[CLS]', 'what', 'is', 'name', '[MASK]', '?', '[SEP]']\n",
            "testing ['?'] at 5 in sentence ['[CLS]', 'what', 'is', 'name', 'your', '[MASK]', '[SEP]']\n",
            "prob1: 0.289340392786225, prob2: 7.71620475199385e-11\n"
          ]
        }
      ]
    }
  ]
}